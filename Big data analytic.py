# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hh40oTpbvV5QJXOtYWOQwZlyNcHUqGA1
"""

!pip install pyspark

import pandas as pd
import random
import time

# Generate synthetic transaction data
def generate_synthetic_data(output_file='transactions.csv', num_records=10000):
    data = [
        {
            "transaction_id": i,
            "customer_id": random.randint(1, 500),
            "merchant_id": random.randint(1, 100),
            "amount": round(random.uniform(1.0, 500.0), 2),
            "latitude": round(random.uniform(-90, 90), 6),
            "longitude": round(random.uniform(-180, 180), 6),
            "timestamp": time.time() - random.randint(0, 100000),
        }
        for i in range(num_records)
    ]

    # Create a pandas DataFrame
    df = pd.DataFrame(data)

    # Save to a CSV file
    df.to_csv(output_file, index=False)
    print(f"Generated {num_records} records and saved to {output_file}")

# Run the function if executed directly
if __name__ == "__main__":
    generate_synthetic_data()

from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("FraudDetection").getOrCreate()

!pip install findspark

!pip install confluent

!pip install kafka

import findspark
findspark.init()

import pandas as pd
import random
import time

# Generate synthetic data

data = [
    {
        "transaction_id": i,
        "customer_id": random.randint(1, 500),
        "merchant_id": random.randint(1, 100),
        "amount": random.uniform(1.0, 500.0),
        "latitude": random.uniform(-90, 90),
        "longitude": random.uniform(-180, 180),
        "timestamp": time.time() - random.randint(0, 100000),
    }
    for i in range(10000)
]

df = pd.DataFrame(data)
df.to_csv("transactions.csv", index=False)

# LOAD DATA INTO PYSPARK
# Read CSV Data

schema = (
    "transaction_id INT, customer_id INT, merchant_id INT, "
    "amount FLOAT, latitude FLOAT, longitude FLOAT, timestamp FLOAT"
)

transactions = spark.read.csv("transactions.csv", schema=schema, header=True)
transactions.show(5)

# PREPROCESS DATA
# Add derived columns and clean data

from pyspark.sql.functions import col, unix_timestamp, from_unixtime

# Convert timestamp to readable format
transactions = transactions.withColumn(
    "timestamp", from_unixtime(col("timestamp"))

)

# Add transaction velocity (Dummy feature)
transactions = transactions.withColumn("transaction_velocity", col("amount") / 100)
transactions.show(5)

# FEATURE ENGINEERING
# Calculate Behavioral Features
# Using spark's window functions to create aggregated metrics

from pyspark.sql.window import Window
from pyspark.sql.functions import avg, count

# Defines a window specification for calcculating rolling metrics
window_spec = Window.partitionBy("customer_id").orderBy("timestamp").rowsBetween(-10, 0)

# Adding new columns 'avg_amount' and 'transaction_count'
transactions = transactions.withColumn(
    "avg_amount", avg("amount").over(window_spec)
).withColumn(
    "transaction_count", count("transaction_id").over(window_spec)
)

transactions.show(5)

# ANOMALY DETECTION
# Prepare Features for Model
# Using PySpark MLlib to assemble features for modeling

from pyspark.ml.feature import VectorAssembler

# List of column names used as input features for ML
feature_cols = ["amount", "transaction_velocity", "avg_amount", "transaction_count"]

# Combines the selected feature columns into single vector
assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")

# Creates new data frame 'features'
feature_data = assembler.transform(transactions)

# Trains KMeans Model
# Train an unsupervised model to detect anomalies

from pyspark.ml.clustering import KMeans

kmeans = KMeans(k=2, seed=1)
model = kmeans.fit(feature_data)

# Applies the trained KMeans model to the feature data
predictions = model.transform(feature_data)
predictions.show(5)

# Filter Suspicious Transactions
# Identify transactions that are flagged as anomalies

suspicious_transactions = predictions.filter(predictions.prediction == 1)
suspicious_transactions.show(5)

# Step 1: Convert to Pandas manually
data = suspicious_transactions.select("latitude", "longitude").collect()
pandas_df = pd.DataFrame([(row.latitude, row.longitude) for row in data], columns=["latitude", "longitude"])

# Step 2: Plot directly from pandas_df
import matplotlib.pyplot as plt

plt.scatter(pandas_df["latitude"], pandas_df["longitude"], alpha=0.5)
plt.title("Geographical Distribution of Suspicious Transactions")
plt.xlabel("Latitude")
plt.ylabel("Longitude")
plt.show()

